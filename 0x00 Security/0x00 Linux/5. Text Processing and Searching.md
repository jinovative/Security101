### 목표

- 로그·CSV·일반 텍스트에서 필요한 정보만 추출·가공
    
- `grep`, `sed`, `awk`, `cut` 기본 사용법 숙달
    

---

### 주요 명령어 & 해석

- **`grep <패턴> <파일>`**
    
    - (global regular expression print)
        
    - **기능**: `<파일>`에서 정규표현식 `<패턴>` 과 일치하는 모든 줄을 출력
        
- **`sed '<명령>' <파일>`**
    
    - (stream editor)
        
    - **기능**: 파일을 수정하지 않고, 스트림 형태로 텍스트 치환·삭제 등 편집
        
    - 예: `sed 's/foo/bar/g' file.txt` → foo→bar로 전역 치환
        
- **`awk '<조건>{액션}' <파일>`**
    
    - (Aho, Weinberger, Kernighan)
        
    - **기능**: 각 줄을 필드(field) 단위로 분리해 `<조건>` 만족 시 `{액션}` 실행
        
    - 예: `awk '{print $1,$3}' file.txt` → 1·3번째 칼럼만 출력
        
- **`cut -d'<구분자>' -f<필드번호> <파일>`**
    
    - **기능**: 지정한 구분자(`-d`)로 필드를 나눠, 원하는 번호(`-f`)만 추출
        
    - 예: `cut -d',' -f2 file.csv` → CSV의 두 번째 컬럼
        

---

### 실습

1. **Apache 접근 로그에서 “200” 상태 코드만 보기**
    
    `grep ' 200 ' /var/log/apache2/access.log > http_200.txt`
    
    - **해석**: `access.log`에서 “ 200 ” (정상 응답) 줄만 `http_200.txt`로 저장
        
2. **로그파일에서 IP 주소 치환하기**
    
    `sed 's/192\.168\.1\.[0-9]\+/LAN-IP/g' http_200.txt > anonymized.txt`
    
    - **해석**: `LAN-IP`로 가상의 IP 치환, 개인정보 보호용 처리
        
3. **CSV 파일에서 사용자 이름과 이메일만 추출**
    
    `cut -d',' -f2,4 users.csv > subset.csv`
    
    - **해석**: `users.csv`의 2번(name)·4번(email) 필드만 `subset.csv`로 저장
        
4. **로그에서 가장 자주 요청된 URL TOP 10**
    
    `awk '{print $7}' /var/log/apache2/access.log \   | sort | uniq -c | sort -nr | head -10`
    
    - **해석**:
        
        1. `$7` 필드(URL) 추출 →
            
        2. `sort`로 정렬 →
            
        3. `uniq -c`로 중복 개수 집계 →
            
        4. 숫자 역순 정렬 →
            
        5. 상위 10개 출력
            
5. **특정 날짜 이후 로그만 필터링**

    `awk '$4 >= "[10/Oct/2023:00:00:00"' /var/log/apache2/access.log > recent.log`
    
    - **해석**: 2023년 10월 10일 자정 이후(`$4`는 타임스탬프) 요청만 `recent.log`로 저장